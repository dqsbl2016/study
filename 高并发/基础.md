# 基础

## 线程安全

多线程操作共享数据时产生的问题。 读写的数据实时性。

## 层次

* 线程安全  线程竞争产生的问题 免于竞争产生的问题
* 条件安全    不同的线程可以同时访问不同的对象，并且对共享数据的访问不受竞争条件的限制。
* 非线程安全  数据结构不应该同时被多线程访问

## CPU 高速缓存

由于计算机的存储设备与处理器的运算速度差距非常大，所以现代计算机系统都会增加一层读写速度尽可能接近处理器运算速度的高速缓存来作为内存和处理器之间的缓冲：将运算需要使用的数据复制到缓存中，让运算能快速进行，当运算结束后再从缓存同步到内存之中。 

现在大部分的处理器都有二级或者三级缓存，从下到上依次为 L3 cache, L2 cache, L1 cache. 缓存又可以分为指令缓存和数据缓存，指令缓存用来缓存程序的代码，数据缓存用来缓存程序的数据。

![1554272291759](D:\study\高并发\img\c1.png)

* L1 Cache，一级缓存，本地 core 的缓存，分成 32K 的数据缓存 L1d 和 32k 指令缓存 L1i，访问 L1 需要 3cycles，耗时大约 1ns；
* L2 Cache，二级缓存，本地 core 的缓存，被设计为 L1 缓存与共享的 L3 缓存之间的缓冲，大小为 256K，访问 L2 需要 12cycles，耗时大约 3ns；
* L3 Cache，三级缓存，在同插槽的所有 core 共享 L3 缓存，分为多个 2M 的段，访问 L3 需要 38cycles，耗时大约 12ns；

## 缓存一致性

CPU-0 读取主内存的数据，缓存到 CPU-0 的高速缓存中，CPU-1 也做了同样的事情，而 CPU-1 把 count 的值修改成了 2，并且同步到 CPU-1 的高速缓存，但是这个修改以后的值并没有写入到主存中，CPU-0 访问该字节，由于缓存没有更新，所以仍然是之前的值，就会导致数据不一致的问题引发这个问题的原因是因为多核心 CPU 情况下存在指令并行执行，而各个CPU 核心之间的数据不共享从而导致缓存一致性问题，为了解决这个问题，CPU 生产厂商提供了相应的解决方案

### 总线锁

当一个 CPU 对其缓存中的数据进行操作的时候，往总线中发送一个 Lock 信号。其他处理器的请求将会被阻塞，那么该处理器可以独占共享内存。总线锁相当于把 CPU 和内存之间的通信锁住了，所以这种方式会导致 CPU 的性能下降，所以 P6 系列以后的处理器，出现了另外一种方式，就是缓存锁。

### 缓存锁

如果缓存在处理器缓存行中的内存区域在 LOCK 操作期间被锁定，当它执行锁操作回写内存时，处理不在总线上声明 LOCK 信号，而是修改内部的缓存地址，然后通过缓存一致性机制来保证操作的原子性，因为缓存一致性机制会阻止同时修改被两个以上处理器缓存的内存区域的数据，当其他处理器回写已经被锁定的缓存行的数据时会导致该缓存行无效。所以如果声明了 CPU 的锁机制，会生成一个 LOCK 指令，会产生两个作用

* Lock 前缀指令会引起引起处理器缓存回写到内存，在 P6 以后的处理器中，LOCK 信号一般不锁总线，而是锁缓存

* 一个处理器的缓存回写到内存会导致其他处理器的缓存无效

### 缓存一致性协议

处理器上有一套完整的协议，来保证 Cache 的一致性，比较经典的应该就是MESI 协议了，它的方法是在 CPU 缓存中保存一个标记位，这个标记为有四种状态（**`MIES`**）

* M(Modified) 修改缓存，当前 CPU 缓存已经被修改，表示已经和内存中的数据不一致了

* I(Invalid) 失效缓存，说明 CPU 的缓存已经不能使用了
* E(Exclusive) 独占缓存，当前 cpu 的缓存和内存中数据保持一直，而且其他处理器没有缓存该数据

* S(Shared) 共享缓存，数据和内存中数据一致，并且该数据存在多个 cpu缓存中。

每个 Core 的 Cache 控制器不仅知道自己的读写操作，也监听其它 Cache 的读写操作，嗅探（snooping）"协议。CPU 的读取会遵循几个原则

*   一个处于M状态的缓存行，必须时刻监听所有试图读取该缓存行对应的主存地址的操作，如果监听到，则必须在此操作执行前把其缓存行中的数据写回CPU。
* 一个处于S状态的缓存行，必须时刻监听使该缓存行无效或者独享该缓存行的请求，如果监听到，则必须把其缓存行状态设置为I。
* 一个处于E状态的缓存行，必须时刻监听其他试图读取该缓存行对应的主存地址的操作，如果监听到，则必须把其缓存行状态设置为S。

当CPU需要读取数据时，如果其缓存行的状态是I的，则需要从内存中读取，并把自己状态变成S，如果不是I，则可以直接读取缓存中的值，但在此之前，必须要等待其他CPU的监听结果，如其他CPU也有该数据的缓存且状态是M，则需要等待其把缓存更新到内存之后，再读取。

       当CPU需要写数据时，只有在其缓存行是M或者E的时候才能执行，否则需要发出特殊的RFO指令(Read Or Ownership，这是一种总线事务)，通知其他CPU置缓存无效(I)，这种情况下性能开销是相对较大的。在写入完成后，修改其缓存状态为M。

       所以如果一个变量在某段时间只被一个线程频繁地修改，则使用其内部缓存就完全可以办到，不涉及到总线事务，如果缓存一会被这个CPU独占、一会被那个CPU 独占，这时才会不断产生RFO指令影响到并发性能。这里说的缓存频繁被独占并不是指线程越多越容易触发，而是这里的CPU协调机制，这有点类似于有时多线程并不一定提高效率，原因是线程挂起、调度的开销比执行任务的开销还要大，这里的多CPU也是一样，如果在CPU间调度不合理，也会形成RFO指令的开销比任务开销还要大。当然，这不是编程者需要考虑的事，操作系统会有相应的内存地址的相关判断

​	并非所有情况都会使用缓存一致性的，如被操作的数据不能被缓存在CPU内部或操作数据跨越多个缓存行(状态无法标识)，则处理器会调用总线锁定;另外当CPU不支持缓存锁定时，自然也只能用总线锁定了，比如说奔腾486以及更老的CPU。

## 实现方式

* 重进入    当一个线程进入临界区时，只要获取到锁后可重复性的进入。
  * ReentrantLock  可重入锁
* 线程本地存储
  * ThreadLocal  
* 不可变对象   
  * java 9以上  Set.class
  * final （但是反射时可改变这个修饰的值）
* 互斥
  * 锁    会造成死锁  资源饥饿等情况
* 原子性
  * 
* 可见性
  * Happen - Before   保证的是可见性
  * volatile 
    * jvm的层面上在修饰的内容前加了 lock指令
* 同步
  * Synchronization   同步进程或 同步数据   java中的不一定是互斥的
  * 死锁 饥饿  优先级倒置  繁忙等待(活锁)



## 同步实现

* 信号量   
  * Semaphore.java ??
* 内存屏障
  * CyclicBarrier.java ??
* 互斥
  * 可能包括一个点 或 一块代码（临界区）
* 条件变量
* 自旋锁
  * Thread   java9中有自旋锁实现  静态方法OnSpinWait() 
* 读-写锁
  * ReentrantReadWriteLock  读写锁（共享/独占模式）



## 名词

* 临界区
* 锁 （互斥）  互斥的结果就是 串行的执行

## 同步原语 Synchronization   

* 锁类型
  * 对象锁 
    *   synchronized 加到 非static 方法 
    * 直接锁一个对象  object等类型
    * 锁this
    * synchronized修饰代码块 
  * 类锁  
    * synchronized 加到 static 方法 
    * 直接锁当前class本身  ***.class /

> Monitor - > Object   Monitor只会读Object对象

* 重进入锁  防止死锁  计算进入次数
* 方法Flags： 方法签名  类文件结构中有   是不是public  是不是静态  是不是有synchronized 修饰 ACC_synchronized 
* 字节码 
  * monitorenter   
  * monitorexit
* 锁实现
  * Thin Lock  廋锁
  * Inflated
  * HeavyWeight

